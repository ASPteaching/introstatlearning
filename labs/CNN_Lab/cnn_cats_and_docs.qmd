---
title: "Convolutional Neural Networks - Lab"
date: "`r Sys.Date()`"
format:
    html: 
      toc: true
      toc-depth: 3
      code-fold: false
      fig-width: 8
      fig-height: 6
    pdf: default
knit:
  quarto:
    chunk_options:
      echo: true
      cache: false
      prompt: false
      tidy: true
      comment: NA
      message: false
      warning: false
    knit_options:
      width: 75
reference-location: margin
execute:
    echo: true
    message: false
    warning: false
    cache: true
bibliography: "../DeepLearning.bib"
editor_options: 
  
  chunk_output_type: console
---


```{r setup, include=T}
library(keras)
```


The *Dogs vs. Cats* dataset that you’ll use isn’t packaged with Keras. 

It was made available by Kaggle as part of a computer-vision competition in late 2013, back when
convnets weren’t mainstream. You can download the original dataset from [https://www.kaggle.com/competitions/dogs-vs-cats/data](https://www.kaggle.com/competitions/dogs-vs-cats/data). 

The pictures are medium-resolution color JPEGs.

Unsurprisingly, the cats-versus-dogs Kaggle competition in 2013 was won by entrants who used convolutional networks. The best entries achieved up to 95% accuracy. 

In this example, you’ll get fairly close to this accuracy (in the next section), even though you’ll be training your models on less than 10% of the data that was available to the competitors.

This dataset contains 25,000 images of dogs and cats (12,500 from each class) and is 543 MB (compressed).

After downloading and un-compressing it, you’ll create a new dataset containing three subsets: a training set with 1,000 samples of each class, a validation set with 500 samples of each class, and a test set with 500 samples of each class.

Keras includes a number of image processing helper tools. In particular, it includes the *image_data_generator()* function, which can automatically turn image files on disk into batches of pre-processed tensors.

```{r}
base_dir<-"cats_and_dogs"

train_dir<- file.path(base_dir,"train",fsep ="/")
validation_dir<-file.path(base_dir,"validation",fsep ="/")
test_dir<-file.path(base_dir,"test",fsep ="/")

train_cats_dir<-file.path(train_dir,"cats128",fsep ="/")
train_dogs_dir<-file.path(train_dir,"dogs128",fsep ="/")

validation_cats_dir<-file.path(validation_dir,"cats128",fsep ="/")
validation_dogs_dir<-file.path(validation_dir,"dogs128",fsep ="/")

test_cats_dir<-file.path(test_dir,"cats128",fsep ="/")
test_dogs_dir<-file.path(test_dir,"dogs128",fsep ="/")
```

# CNN definition

![](cnn.jpg){width=100%}




```{r}
model <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32, kernel_size = c(3, 3),
                activation = "relu", 
                input_shape = c(128, 128, 3)) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_flatten() %>%
  layer_dense(units = 512, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")
```

```{r}
summary(model)
```


```{r}
# image_data_generator Generate batches of image data with real-time data augmentation. 
# The data will be looped over (in batches).
train_datagen <- image_data_generator(rescale = 1/255)  #
validation_datagen <- image_data_generator(rescale = 1/255) #

# flow_images_from_directory Generates batches of data from images in a directory (with optional augmented/normalized data)
train_generator <- flow_images_from_directory(
  train_dir,
  train_datagen,
  target_size = c(128, 128),
  batch_size = 20,
  class_mode = "binary"
)
validation_generator <- flow_images_from_directory(
  validation_dir,
  validation_datagen,
  target_size = c(128, 128),
  batch_size = 20,
  class_mode = "binary"
)

batch <- generator_next(train_generator)
str(batch)   # assigns image labels through the number of subdirectories (cat and dogs)
```

Let’s look at the output of one of these generators: it yields batches of 128 × 128.

RGB images (shape (20, 128, 128, 3)) and binary labels (shape (20)). 

There are 20 samples in each batch (the batch size). Note that the generator yields these batches
indefinitely: it loops endlessly over the images in the target folder.

```{r}
model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(lr = 1e-4),
  metrics = c("accuracy")
)
```


Let’s fit the model to the data using the generator. You do so using the
`fit_generator` function, the equivalent of fit for data generators like this one. 

It expects as its first argument a generator that will yield batches of inputs and targets
indefinitely, like this one does. Because the data is being generated endlessly, the
generator needs to know how many samples to draw from the generator before declaring
an epoch over. 

This is the role of the `steps_per_epoch` argument: after having drawn steps_per_epoch batches from the generator—that is, after having run for
steps_per_epoch gradient descent steps—the fitting process will go to the next epoch.
In this case, batches are 20-samples large, so it will take 100 batches until you see your
target of 2,000 samples.
When using fit_generator, you can pass a validation_data argument, much as
with the fit function. It’s important to note that this argument is allowed to be a data
generator, but it could also be a list of arrays. If you pass a generator as
validation_data, then this generator is expected to yield batches of validation data
endlessly; thus you should also specify the validation_steps argument, which tells the
process how many batches to draw from the validation generator for evaluation.

```{r}
history <- model %>% fit_generator(
  train_generator,
  steps_per_epoch = 100,  #100
  epochs = 5,  # 20
  validation_data = validation_generator,
  validation_steps = 50  #50
)
```

```{r}
model %>% save_model_hdf5("cats_and_dogs_small_1.h5")
```

```{r}
model <- load_model_hdf5("cats_and_dogs_small_1.h5")
```

```{r}
plot(history) 
```

These plots are characteristic of overfitting. 
The training accuracy increases linearly
over time, until it reaches nearly 100%, whereas the validation accuracy stalls at 71–75%.
The validation loss reaches its minimum after only five epochs and then stalls, whereas
the training loss keeps decreasing linearly until it reaches nearly 0.
Because you have relatively few training samples (2,000), overfitting will be your
number-one concern. You already know about a number of techniques that can help
mitigate overfitting, such as dropout and weight decay (L2 regularization). We’re now
going to introduce a new one, specific to computer vision and used almost universally
when processing images with deep-learning models: data augmentation.


# Using data augmentation

Overfitting is caused by having too few samples to learn from, rendering you unable to
train a model that can generalize to new data. Given infinite data, your model would be
exposed to every possible aspect of the data distribution at hand: you would never
overfit. Data augmentation takes the approach of generating more training data from
existing training samples, by augmenting the samples via a number of random
transformations that yield believable-looking images. The goal is that at training time,
your model will never see the exact same picture twice. This helps expose the model to
more aspects of the data and generalize better.
In Keras, this can be done by configuring a number of random transformations to be
performed on the images read by an image_data_generator. Let’s get started with an
example.

These are just a few of the options available (for more, see the Keras documentation).
Let’s quickly go over this code:


* rotation_range is a value in degrees (0–180), a range within which to randomly rotate
pictures.
* width_shift and height_shift are ranges (as a fraction of total width or height) within which to randomly translate pictures vertically or horizontally.
* shear_range is for randomly applying shearing transformations.
* zoom_range is for randomly zooming inside pictures.
* horizontal_flip is for randomly flipping half the images horizontally—relevant when
there are no assumptions of horizontal asymmetry (for example, real-world pictures).
* fill_mode is the strategy used for filling in newly created pixels, which can appear after
a rotation or a width/height shift.

```{r}
datagen <- image_data_generator(
  rescale = 1/255,
  rotation_range = 40,
  width_shift_range = 0.2,
  height_shift_range = 0.2,
  shear_range = 0.2,
  zoom_range = 0.2,
  horizontal_flip = TRUE,
  fill_mode = "nearest",
)
```


```{r}
fnames <- list.files(train_cats_dir, full.names = TRUE)
img_path <- fnames[[3]]
img <- image_load(img_path, target_size = c(128, 128))
img_array <- image_to_array(img)
img_array <- array_reshape(img_array, c(1, 128, 128, 3))
augmentation_generator <- flow_images_from_data(
 img_array,
 generator = datagen,
 batch_size = 1
)
op <- par(mfrow = c(2, 2), pty = "s", mar = c(1, 0, 1, 0))
for (i in 1:4) {
 batch <- generator_next(augmentation_generator)   # Use to retrieve items from generators
 plot(as.raster(batch[1,,,]))
}
par(op)
```

```{r}
datagen <- image_data_generator(
  rescale = 1/255,
  rotation_range = 40,
  width_shift_range = 0.2,
  height_shift_range = 0.2,
  shear_range = 0.2,
  zoom_range = 0.2,
  horizontal_flip = TRUE,
  fill_mode = "nearest"
)


train_generator <- flow_images_from_directory(
  train_dir,
  datagen,
  target_size = c(128, 128),
  batch_size = 20,
  class_mode = "binary"
)

validation_datagen <- image_data_generator(rescale = 1/255)

validation_generator <- flow_images_from_directory(
  validation_dir,
  validation_datagen,
  target_size = c(128, 128),
  batch_size = 20,
  class_mode = "binary"
)

batch <- generator_next(train_generator)
str(batch)   
```

```{r}
model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(lr = 1e-4),
  metrics = c("acc")
)
```

```{r}
history <- model %>% fit_generator(
  train_generator,
  steps_per_epoch = 100, #100,
  epochs = 5,  # 20
  validation_data = validation_generator,
  validation_steps = 50 #50
)
```

```{r}
model %>% save_model_hdf5("cats_and_dogs_small_2.h5")
```

```{r}
model <- load_model_hdf5("cats_and_dogs_small_2.h5")
```


```{r}
plot(history) 
```


# Using a pretrained convnet


A common and highly effective approach to deep learning on small image datasets is to use a pretrained network. 

A pretrained network is a saved network that was previously trained on a large dataset, typically on a large-scale image-classification task. 

If this original dataset is large enough and general enough, then the spatial-feature hierarchy
learned by the pretrained network can effectively act as a generic model of the visual
world, and hence its features can prove useful for many different computer-vision
problems, even though these new problems may involve completely different classes than
those of the original task. For instance, you might train a network on ImageNet (where
classes are mostly animals and everyday objects) and then repurpose this trained network
for something as remote as identifying furniture items in images. Such portability of
learned features across different problems is a key advantage of deep learning compared
to many older, shallow-learning approaches, and it makes deep learning very effective for
small-data problems.

In this case, let’s consider a large convnet trained on the ImageNet dataset 
(1.4 million labeled images and 1,000 different classes). ImageNet contains many animal
classes, including different species of cats and dogs, and you can thus expect to perform
well on the cats-versus-dogs classification problem.
You’ll use the VGG16 architecture, developed by Karen Simonyan and Andrew
Zisserman in 2014; it’s a simple and widely used convnet architecture for ImageNet.8
Although it’s an older model, far from the current state of the art and somewhat heavier
than many other recent models, we chose it because its architecture is similar to what
you’re already familiar with and is easy to understand without introducing any new
concepts.

```{r}
conv_base <- application_vgg16(
  weights = "imagenet",
  include_top = FALSE,
  input_shape = c(128, 128, 3)
)
```

```{r}
conv_base
```

```{r}
model <- keras_model_sequential() %>%
  conv_base %>%
  layer_flatten() %>%
  layer_dense(units = 256, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

summary(model)
```

```{r}
cat("This is the number of trainable weights before freezing",
    "the conv base:", length(model$trainable_weights), "\n")

freeze_weights(conv_base)   # Note that in order for these changes to take effect, you must recompile the model.


cat("This is the number of trainable weights after freezing",
    "the conv base:", length(model$trainable_weights), "\n")
```


```{r}
summary(model)
```



```{r}
train_datagen = image_data_generator(
  rescale = 1/255,
  rotation_range = 40,
  width_shift_range = 0.2,
  height_shift_range = 0.2,
  shear_range = 0.2,
  zoom_range = 0.2,
  horizontal_flip = TRUE,
  fill_mode = "nearest"
)
```


```{r}
train_generator <- flow_images_from_directory(
  train_dir,
  train_datagen,
  target_size = c(128, 128),
  batch_size = 20,
  class_mode = "binary"
)
```


```{r}
validation_datagen <- image_data_generator(rescale = 1/255)
```

```{r}
validation_generator <- flow_images_from_directory(
  validation_dir,
  validation_datagen,
  target_size = c(128, 128),
  batch_size = 20,
  class_mode = "binary"
)
```

```{r}
model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(lr = 2e-5),
  metrics = c("accuracy")
) 
```


```{r, echo=F, include=F}
history <- model %>% fit_generator(
  train_generator,
  steps_per_epoch = 100,  #100
  epochs = 5,  # 20
  validation_data = validation_generator,
  validation_steps = 50   #50
)
```


```{r}
model %>% save_model_hdf5("cats_and_dogs_small_3.h5")
```


```{r}
#model <- load_model_hdf5("cats_and_dogs_small_3.h5")
```



```{r}
plot(history) 
```


# Fine-tuning


Fine-tuning consists of unfreezing a few of the top layers of a frozen model base used for feature extraction, and jointly training both the newly added
part of the model (in this case, the fully connected classifier) and these top layers. This is
called fine-tuning because it slightly adjusts the more abstract representations of the
model being reused, in order to make them more relevant for the problem at hand.

```{r}
unfreeze_weights(conv_base,from ="block3_conv1")
summary(model)
```

```{r}
#model %>% compile(
#  loss = "binary_crossentropy",
#  optimizer = optimizer_rmsprop(lr = 2e-5),
#  metrics = c("accuracy")
#)
```


```{r}
#history <- model %>% fit_generator(
#  train_generator,
#  steps_per_epoch = 100,
#  epochs = 30,
#  validation_data = validation_generator,
#  validation_steps = 50
#)
```


# Feature extraction


```{r}
datagen <- image_data_generator(rescale = 1/255)
batch_size <- 20

extract_features <- function(directory, sample_count) {
features <- array(0, dim = c(sample_count, 4, 4, 512))
labels <- array(0, dim = c(sample_count))
    generator <- flow_images_from_directory(
    directory = directory,
    generator = datagen,
    target_size = c(128, 128),
    batch_size = batch_size,
    class_mode = "binary"
 )
 i <- 0
 while(TRUE) {
 batch <- generator_next(generator)
 inputs_batch <- batch[[1]]
 labels_batch <- batch[[2]]
 features_batch <- conv_base %>% predict(inputs_batch)
 index_range <- ((i * batch_size)+1):((i + 1) * batch_size)
 features[index_range,,,] <- features_batch
 labels[index_range] <- labels_batch
 i <- i + 1
 if (i * batch_size >= sample_count)
 break
 }
 list(
 features = features,
 labels = labels
 )
}


train <- extract_features(train_dir, 1000)
validation <- extract_features(validation_dir, 500)
test <- extract_features(test_dir, 500)
```

```{r}
reshape_features <- function(features) {
 array_reshape(features, dim = c(nrow(features), 4 * 4 * 512))
}
train$features <- reshape_features(train$features)
dim(train$features)
validation$features <- reshape_features(validation$features)
dim(validation$features)
test$features <- reshape_features(test$features)
dim(test$features)
```


```{r}
model <- keras_model_sequential() %>%
 layer_dense(units = 256, activation = "relu",
 input_shape = 4 * 4 * 512) %>%
 layer_dropout(rate = 0.5) %>%
 layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
 optimizer = optimizer_rmsprop(lr = 2e-5),
 loss = "binary_crossentropy",
 metrics = c("accuracy")
)

history <- model %>% fit(
 train$features, train$labels,
 epochs = 5,  # 20
 batch_size = 20,
 validation_data = list(validation$features, validation$labels)
)
```
```{r}
summary(model)
```


```{r}
plot(history) 
```


# Visualizing what convnets learn

## Visualizing intermediate activations

```{r}
model <- load_model_hdf5("cats_and_dogs_small_2.h5")
model

img<-image_load(paste0(test_cats_dir,"/","cats1512.jpg"))
img_tensor<-image_to_array(img)
img_tensor<-array_reshape(img_tensor,c(1,128,128,3))
img_tensor<-img_tensor/255
dim(img_tensor)
plot(as.raster(img_tensor[1,,,]))
```
Extracts the outputs of the top eight layers and creates a model that will return these outputs, given the model input.

```{r}
layer_outputs <- lapply(model$layers[1:8], function(layer) layer$output)
activation_model <- keras_model(inputs = model$input, outputs = layer_outputs)
activations <- activation_model %>% predict(img_tensor)

first_layer_activation <- activations[[1]]
dim(first_layer_activation)
```

Compare with summary of the model


```{r}
plot_channel <- function(channel) {
rotate <- function(x) t(apply(x, 2, rev))
image(rotate(channel), axes = FALSE, asp = 1,
col = terrain.colors(12))
}

plot_channel(first_layer_activation[1,,,1])
plot_channel(first_layer_activation[1,,,5])
plot_channel(first_layer_activation[1,,,15])
plot_channel(first_layer_activation[1,,,20])
```


```{r}
# access other layers
layer_activation <- activations[[8]]
dim(layer_activation)
plot_channel(layer_activation[1,,,1])
plot_channel(layer_activation[1,,,50])
plot_channel(layer_activation[1,,,120])
```


Make predictions

```{r}
#img<-image_load(paste0(test_cats_dir,"/","cats1574.jpg"))
img<-image_load(paste0(test_dogs_dir,"/","dogs1574.jpg"))
img_tensor<-image_to_array(img)
img_tensor<-array_reshape(img_tensor,c(1,128,128,3))
img_tensor<-img_tensor/255
dim(img_tensor)
plot(as.raster(img_tensor[1,,,]))
```



```{r}
model %>% predict(img_tensor)
```


