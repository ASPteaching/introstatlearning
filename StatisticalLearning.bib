@book{james2013introduction,
  title={An introduction to statistical learning},
  author={James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  volume={112},
  year={2013},
  publisher={Springer}
}
@misc{amat2017,
   author = {Joaquín Amat Rodrigo},
   title = {Arboles de decision, Random Forest, Gradient Boosting y C5.0},
   url = {https://www.cienciadedatos.net/documentos/33_arboles_de_prediccion_bagging_random_forest_boosting#Introducci%C3%B3n},
   year = {2017},
}

@article{Breiman1996,
   abstract = {Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy. © 1996 Kluwer Academic Publishers,.},
   author = {Leo Breiman},
   doi = {10.1007/BF00058655/METRICS},
   issn = {08856125},
   issue = {2},
   journal = {Machine Learning},
   keywords = {Aggregation,Averaging,Bootstrap,Combining},
   pages = {123-140},
   publisher = {Springer Netherlands},
   title = {Bagging predictors},
   volume = {24},
   url = {https://link.springer.com/article/10.1007/BF00058655},
   year = {1996},
}
@article{Efron79,
author = {B. Efron},
title = {{Bootstrap Methods: Another Look at the Jackknife}},
volume = {7},
journal = {The Annals of Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {1 -- 26},
keywords = {bootstrap, discriminant analysis, error rate estimation, jackknife, Nonlinear regression, nonparametric variance estimation, Resampling, subsample values},
year = {1979},
doi = {10.1214/aos/1176344552},
URL = {https://doi.org/10.1214/aos/1176344552}
}

@article{Hastie2009,
   author = {Trevor Hastie and Robert Tibshirani and Jerome Friedman},
   city = {New York, NY},
   doi = {10.1007/978-0-387-84858-7},
   isbn = {978-0-387-84857-0},
   publisher = {Springer New York},
   title = {The Elements of Statistical Learning},
   url = {http://link.springer.com/10.1007/978-0-387-84858-7},
   year = {2009},
}
@article{Genuer2020,
   author = {Robin Genuer and Jean-Michel Poggi},
   city = {Cham},
   doi = {10.1007/978-3-030-56485-8},
   isbn = {978-3-030-56484-1},
   publisher = {Springer International Publishing},
   title = {Random Forests with R},
   url = {http://link.springer.com/10.1007/978-3-030-56485-8},
   year = {2020},
}
@article{Boulesteix2012,
   abstract = {The random forest (RF) algorithm by Leo Breiman has become a standard data analysis tool in bioinformatics. It has shown excellent performance in settings where the number of variables is much larger than the number of observations, can cope with complex interaction structures as well as highly correlated variables and return measures of variable importance. This paper synthesizes 10 years of RF development with emphasis on applications to bioinformatics and computational biology. Special attention is paid to practical aspects such as the selection of parameters, available RF implementations, and important pitfalls and biases of RF and its variable importance measures (VIMs). The paper surveys recent developments of themethodology relevant to bioinformatics as well as some representative examples of RF applications in this context and possible directions for future research. © 2012 Wiley Periodicals, Inc.},
   author = {Anne Laure Boulesteix and Silke Janitza and Jochen Kruppa and Inke R. König},
   doi = {10.1002/WIDM.1072},
   issn = {19424795},
   issue = {6},
   journal = {undefined},
   month = {11},
   pages = {493-507},
   publisher = {Wiley-Blackwell},
   title = {Overview of random forest methodology and practical guidance with emphasis on computational biology and bioinformatics},
   volume = {2},
   year = {2012},
}
@article{Kuhn2013,
   abstract = {Applied Predictive Modeling covers the overall predictive modeling process, beginning with the crucial steps of data preprocessing, data splitting and foundations of model tuning. The text then provides intuitive explanations of numerous common and modern regression and classification techniques, always with an emphasis on illustrating and solving real data problems. The text illustrates all parts of the modeling process through many hands-on, real-life examples, and every chapter contains extensive R code for each step of the process. This multi-purpose text can be used as an introduction to predictive models and the overall modeling process, a practitioner's reference handbook, or as a text for advanced undergraduate or graduate level predictive modeling courses. To that end, each chapter contains problem sets to help solidify the covered concepts and uses data available in the book's R package. This text is intended for a broad audience as both an introduction to predictive models as well as a guide to applying them. Non-mathematical readers will appreciate the intuitive explanations of the techniques while an emphasis on problem-solving with real data across a wide variety of applications will aid practitioners who wish to extend their expertise. Readers should have knowledge of basic statistical ideas, such as correlation and linear regression analysis. While the text is biased against complex equations, a mathematical background is needed for advanced topics.},
   author = {Max Kuhn and Kjell Johnson},
   doi = {10.1007/978-1-4614-6849-3},
   isbn = {9781461468493},
   journal = {Applied Predictive Modeling},
   month = {1},
   pages = {1-600},
   publisher = {Springer New York},
   title = {Applied predictive modeling},
   year = {2013},
}
