PART 1-TREES
1/75. Tree based methods
2/75. Introduction to Decision Trees
3/75. Motivation
4/75. Examples (1)
5/75. Examples (2)
6/75. So, what is a decision tree?
7/75. What do we need to learn?
8/75. More about context
9/75. Types of decision trees
10/75. Classification vs Regression Trees
11/75. Building the trees
12/75. Tree building with R
13/75. A “quick and dirty” example in R
14/75. Predicting Diabetes onset
15/75. A simple, unprunned tree
16/75. How accurate is the model?
17/75. Always use train/test sets!
18/75. Build on train, Estimate on test
19/75. Constructing Trees
20/75. Notation and basic concepts
21/75. Additional notation
22/75. Trees partition the space
23/75. The tree represents the splitting
24/75. Construction of a tree
25/75. Which questions to split the tree
26/75. Determining Goodness of Split
27/75. Good splits vs bad splits
28/75. Measuring homogeneity
29/75. Impurity functions
30/75. Some Impurity Functions
31/75. Impurity measures for a split
32/75. Goodness of a split
33/75. Impurity score for a node
34/75. Applications of \(I(t)\)
35/75. Entropy as an impurity measure
36/75. Goodness of split based on entropy
37/75. Information gain
38/75. Example
39/75. Example. Entropy Calculation
40/75. Example. Information Gain
41/75. When to stop growing
42/75. Class Assignment Rules (1)
43/75. Class Assignment Rules (2)
44/75. Class Assignment Rules (3)
45/75. Estimating the error rate (1)
46/75. Estimating the error rate (2)
47/75. Optimizing the Tree
48/75. Pruning methods
49/75. Cost complexity pruning
50/75. Regression Trees
51/75. Regression modelling with trees
52/75. Regression vs classification trees
53/75. Regression tree example
54/75. Non linear relationships!
55/75. Building the tree (1): Splitting
56/75. Building the tree (2): Splitting
57/75. Building the tree (3): Prediction
58/75. Example: A regression tree
59/75. Example: Plot the tree
60/75. Prunning the tree (1)
61/75. Tuning parameter \(\alpha\)
62/75. Optimizing the tree (\(\alpha\))
63/75. Example: Prune the tree
64/75. Advantages and disadvantages of trees
65/75. Trees have many advantages
66/75. But they come at a price
67/75. References and Resources
68/75. References
69/75. Complementary references
70/75. Resources

PART II: ENSEMBLES
1/70. Ensemble Methods
2/70. Introduction to Ensembles
3/70. Some problems of weak learners.
4/70. Ensembles
5/70. Ensemble methods
6/70. Bagging: Aggregating predictors
7/70. Bagging: bootstrap aggregation
8/70. Averaging decreases variance
9/70. Averaging trees …
10/70. The bootstrap
11/70. Bootstrap Applications
12/70. Precision of an estimate (1)
13/70. Plug-in estimates
14/70. Precision of an estimate (1)
15/70. Precision of an estimate (2)
16/70. Standard error estimation
17/70. The bootstrap (1)
18/70. The bootstrap (2)
19/70. Bootstrap sampling (resampling)
20/70. Bootstrap resampling (2)
21/70. The bootstrap distribution
22/70. Bootstrap Monte Carlo Algorithm
23/70. Bootstrap Estimates of SE
24/70. Summary
25/70. Back to bagging
26/70. Bagging prediction/classifier
27/70. Out-Of-Bag observations
28/70. Out-Of-Bag error estimates
29/70. Illustration of OOB EE
30/70. Bagging in R (1.1)
31/70. Bagging in R (1.2)
32/70. Interpetability: The “achiles heel”
33/70. Variable importance
34/70. Variable importance example
35/70. Random Forests
36/70. Random forests: decorrelating predictors
37/70. Random forests
38/70. How many variables per split?
39/70. Random forest algorithm
40/70. Out-of-the box performance
41/70. Out of the box performance
42/70. Out of the box performance example
43/70. Tuning hyperparameters
44/70. Random forests in bioinformatics
45/70. Application of Random forests
46/70. Boosting
47/70. Another ensemble approach
48/70. Bagging vs Boosting
49/70. So what is Boosting
50/70. Historical background
51/70. Advantages of Boosting
52/70. Limitations of Boosting
53/70. Adaboost
54/70. Adaboost Architecture
55/70. Adaboost pseudo-code
56/70. Adaboost applications
57/70. Adaboost has limitations
58/70. Gradient Boosting
59/70. Gradient boosting algorithm
60/70. Gradient boosting architechture
61/70. Gradient Boosting pseudo-code
62/70. Relation with Gradient Descent
63/70. Gradient Descent Variations
64/70. Boosting applications
65/70. Boosting application with R
66/70. References

PART III: ARTIFICIAL NEURAL NETWORKS

1/99. Introduction to ANN and Deep Learning
2/99. Overview of Deep Learning
3/99. Historical Background (1)
4/99. Historical Background (2)
5/99. Deep learning
6/99. The early history of AI (1)
7/99. Milestones in the history of DL
8/99. From Artificial Neural Networks to Deep learning
9/99. Success stories
10/99. AI, ML, DL …
11/99. AI, ML, DL …
12/99. Machine vs Deep Learning
13/99. Size does matter!
14/99. The impact of Deep learning
15/99. Not all that glitters is gold …
16/99. Artificial Neural Networks
17/99. The perceptron, the building block
18/99. Mc Cullough’s neuron
19/99. Mc Cullough’s neuron
20/99. Limitations
21/99. Overcoming the limitations
22/99. Rosenblatt’s perceptron
23/99. Rosenblatt’s perceptron (1)
24/99. Comparison between the two
25/99. Comparison between the two
26/99. Activation in biological neurons
27/99. Activation functions in AN
28/99. Activation function
29/99. Artificial Neuron
30/99. Activation functions
31/99. The sigmoid function
32/99. the hyperbolic tangent
33/99. The ReLU
34/99. More activation functions
35/99. Putting it all together
36/99. Multilayer perceptrons
37/99. An Artificial Neural network
38/99. The architecture of ANN
39/99. The architecture of an ANN
40/99. Activation functions for ANN (1)
41/99. The soft-max activation function
42/99. An example
43/99. A predictive ANN
44/99. Data for the example
45/99. Data pre-processing
46/99. Test and training sets
47/99. Training a neural network
48/99. Network plot
49/99. Predictions
50/99. Model evaluation
51/99. Some mathematics behind ANN
52/99. Training an ANN
53/99. The tools for training
54/99. A guiding example
55/99. A logistic regression ANN
56/99. Logistic regression (1)
57/99. Logistic regression (2)
58/99. ANN have weights aka parameters
59/99. Back to the example:
60/99. The ANN is defined by its weights
61/99. Combining everything to compute
62/99. Compacting notation
63/99. Compacting notation (II)
64/99. Matricial representation (I)
65/99. Matricial representation (II)
66/99. Forward propagation
67/99. Multiple architectures for ANN
68/99. Multiple layer dense Networks
69/99. Feed Forward neural networks (FFNN)
70/99. A loss function for optimization
71/99. Cross-entropy loss function
72/99. A convex cost function
73/99. A regularized cost function
74/99. Optimization with gradient descent
75/99. How gradient descent works (I)
76/99. How gradient descent works (II)
77/99. How gradient descent works (III)
78/99. How gradient descent works (IV)
79/99. How gradient descent works (V)
80/99. The gradient descent algorithm
81/99. Gradient descent computes derivatives
82/99. Other requirements for gradient descent
83/99. Weights initialization
84/99. Gradient descent drawbacks
85/99. Stochastic Gradient
86/99. Rationale for SGD
87/99. SGD variants
88/99. SGD variants (3)
89/99. Improving SGD
90/99. Back propagation
91/99. The delta rule
92/99. Applying the chain rule
93/99. Back-propagation: Forward Phase:
94/99. Back-propagation: Backward Phase:
95/99. References and Resources


PART IV. CONVOLUTIONAL NETWORKS

1/62. Convolutional Neural Networks
2/62. Introduction
3/62. Session Outline
4/62. What do we mean by computers vision?
5/62. We want computers that can see
6/62. DNN useful in computer vision systems
7/62. Facial detection and recognition
8/62. Autonomous driving
9/62. Medicine, biology. self care
10/62. What computers see?
11/62. Images are numbers
12/62. Main tasks in Computer Vision:
13/62. High level feature detection
14/62. Manual feature extraction
15/62. Manual feature extraction
16/62. Automatic feature extraction
17/62. Learning visual features
18/62. Feature extraction with dense NN
19/62. Accounting for spatial structure
20/62. Extending the idea with patches
21/62. Use filters to extract features
22/62. Feature Extraction with Convolutions
23/62. Example: “X or X”?
24/62. What are tye features of X
25/62. Filters can detect X features
26/62. Is a given patch in the image?
27/62. The Convolution Operation
28/62. The Convolution Operation
29/62. The Convolution Operation
30/62. The Convolution Operation
31/62. Different filters for different patterns
32/62. Can filters be learned?
33/62. Convolutional Neural Networks
34/62. CNNs: Overview
35/62. Convolutional Layers
36/62. Convolutional Layers
37/62. CNNs output volume
38/62. Non linear activation
39/62. Pooling
40/62. Pooling
41/62. Common types of pooling
42/62. Putting CNNs to work
43/62. Summary: CNNs for classification
44/62. Summary: CNNs for classification
45/62. A toy example
46/62. The MNIST dataset
47/62. Data pre-processing (1): Reshaping
48/62. Data pre-processing (2): Other transforms
49/62. Modeling (1): Definition
50/62. Modeling (1): Model Summary
51/62. Modeling (2): Compilation
52/62. Model training
53/62. Model evaluation
54/62. References and Resources
55/62. Resources

PART V. Deep Neural Networks with Keras in R

1/37. Deep Neural Networks with Keras in R
2/37. Deep Learning with R
3/37. Outline
4/37. Software for Deep Learning
5/37. TensorFlow
6/37. Pytorch
7/37. Keras
8/37. Deep learning with R
9/37. The Keras pipeline
10/37. A keras cheat sheet
11/37. Hello world of deep learning (1)
12/37. Hello world of deep learning (2)
13/37. Hello world of deep learning (3)
14/37. Hello world of deep learning (4)
15/37. Hello world of deep learning (5)
16/37. Hello world of deep learning (6)
17/37. Tensors
18/37. One must-do digression: Tensors
19/37. Why tensors?
20/37. One and two dimensional tensors
21/37. Rank three tensors
22/37. Rank four tensors
23/37. Rank five tensors
24/37. One can always reshape
25/37. Hyperparameter tuning
26/37. Tuning hyperparameters of DNN
27/37. The tfruns package
28/37. A tfruns example
29/37. Example: Setting the flags
30/37. Example: changing flag values
31/37. Comparing distinct runs:
32/37. Example: Access comparison results
33/37. References and Resources
34/37. References and Resources
