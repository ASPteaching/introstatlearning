---
title: "Decision Trees with R. Examples"
author: "Alex Sanchez"
format:
  html:
    toc: true
    fig-width: 8
    fig-height: 6
    embed-resources: true
execute:
    echo: false
    message: false
    warning: false
    
editor_options: 
  chunk_output_type: console
---

```{r, echo = FALSE, results = 'hide', message = FALSE}
# include this code chunk as-is to set options
Sys.setlocale("LC_TIME", "C")
```

```{r packages, echo=FALSE, message=FALSE}
if (!require(caret)) install.packages("caret", dep=TRUE)
if (!require(mlbench)) install.packages("mlbench", dep=TRUE)
if (!require(skimr)) install.packages("skimr", dep=TRUE)
if (!require(dplyr)) install.packages("dplyr", dep=TRUE)
```


# Example 1: Introduction

## The Pima Indians dataset

The Pima Indian Diabetes data set (`PimaIndiansDiabetes2`) is available in the `mlbench` package.

The data contains 768 individuals (female) and 9 clinical variables for predicting the probability of individuals in being diabete-positive or negative:

- pregnant: number of times pregnant
- glucose: plasma glucose concentration
- pressure: diastolic blood pressure (mm Hg)
- triceps: triceps skin fold thickness (mm)
- insulin: 2-Hour serum insulin (mu U/ml)
- mass: body mass index (weight in kg/(height in m)^2)
- pedigree: diabetes pedigree function
- age: age (years)
- diabetes: class variable

A typical classification/prediction problem is to build a model that can distinguish and predict diabetes using some or all the variables in the dataset.

A quick exploration can be done wirh the `swirl` package:

```{r}
library(skimr)
data("PimaIndiansDiabetes2", package = "mlbench")
skim(PimaIndiansDiabetes2)
```

## Building a classification tree with `rpart``

Start building a simple tree with default parameters

```{r}
library(rpart)
model1 <- rpart(diabetes ~., data = PimaIndiansDiabetes2)
# par(xpd = NA) # otherwise on some devices the text is clipped
```

This builds a model consisting of a series of nested decision rules.

```{r}
print(model1)
```

The model can be visualized using a tree:

```{r}
plot(model1)
text(model1, digits = 3, cex=0.8)
```

If we believed the model was ready for use we could use it to predict diabetes for new subject.


**Imagine we kow nothing about overfitting**. We may want to check the accuracy of the model on the dataset we have used to build it.

```{r}
predicted.classes<- predict(model1, PimaIndiansDiabetes2, "class")
mean(predicted.classes == PimaIndiansDiabetes2$diabetes)
```

A better strategy is to use train dataset to build the model and a test dataset to check how it works.

```{r}
set.seed(123)
ssize <- nrow(PimaIndiansDiabetes2)
propTrain <- 0.8
training.indices <-sample(1:ssize, floor(ssize*propTrain))
train.data  <- PimaIndiansDiabetes2[training.indices, ]
test.data <- PimaIndiansDiabetes2[-training.indices, ]
```

Now we build the model on the train data and check its accuracy on the test data.

```{r}
model2 <- rpart(diabetes ~., data = train.data)
predicted.classes.test<- predict(model2, test.data, "class")
mean(predicted.classes.test == test.data$diabetes)
```

The accuracy is good, but smaller, as expected.


